{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.chdir('/projects/cft_vlm/finetune')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration, Qwen2_5_VLProcessor\n",
    "import torch\n",
    "\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-3B-Instruct\", use_fast=True)\n",
    "# model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\"Qwen/Qwen2.5-VL-3B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "ds = datasets.load_from_disk(\"/projects/cft_vlm/datasets/path_vqa/data/dataset\")['test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=CMYK size=335x400>,\n",
       "   'question': 'how are the histone subunits charged?',\n",
       "   'answer': 'positively charged'}],\n",
       " [{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=CMYK size=315x337>,\n",
       "   'question': 'what is showing increased eosinophilia of cytoplasm, and swelling of occasional cells?',\n",
       "   'answer': 'early (reversible) ischemic injury'}],\n",
       " [{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=CMYK size=492x324>,\n",
       "   'question': 'does mycobacterium avium infection in a duodenal biopsy from a patient with aids show massive intracellular macrophage infection with acid-fast organisms filamentous and pink in this acid-fast stain preparation?',\n",
       "   'answer': 'yes'}],\n",
       " [{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1076x421>,\n",
       "   'question': 'what shows branching papillae having flbrovascular stalk covered by a single layer of cuboidal cells having ground-glass nuclei?',\n",
       "   'answer': 'microscopy'}]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qwenvl.data.module import DatasetWrapper\n",
    "\n",
    "dw = DatasetWrapper(ds, [[i] for i in range(len(ds))])\n",
    "dw[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "Tried to overwrite /fs01/projects/cft_vlm/datasets/open_pmc/data/dataset/test but a dataset can't overwrite itself.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m ds2 \u001b[38;5;241m=\u001b[39m ds\u001b[38;5;241m.\u001b[39mmap(dummy, num_proc\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m24\u001b[39m)\u001b[38;5;241m.\u001b[39mremove_columns([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     10\u001b[0m shutil\u001b[38;5;241m.\u001b[39mrmtree(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/projects/cft_vlm/datasets/open_pmc/data/dataset/test\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m \u001b[43mds2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_to_disk\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/projects/cft_vlm/datasets/open_pmc/data/dataset/test\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/fs01/projects/cft_vlm/.venv/lib/python3.10/site-packages/datasets/arrow_dataset.py:1493\u001b[0m, in \u001b[0;36mDataset.save_to_disk\u001b[0;34m(self, dataset_path, max_shard_size, num_shards, num_proc, storage_options)\u001b[0m\n\u001b[1;32m   1491\u001b[0m     \u001b[38;5;66;03m# Check that the dataset doesn't overwrite itself. It can cause a permission error on Windows and a segfault on linux.\u001b[39;00m\n\u001b[1;32m   1492\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m Path(dataset_path)\u001b[38;5;241m.\u001b[39mexpanduser()\u001b[38;5;241m.\u001b[39mresolve() \u001b[38;5;129;01min\u001b[39;00m parent_cache_files_paths:\n\u001b[0;32m-> 1493\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mPermissionError\u001b[39;00m(\n\u001b[1;32m   1494\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTried to overwrite \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPath(dataset_path)\u001b[38;5;241m.\u001b[39mexpanduser()\u001b[38;5;241m.\u001b[39mresolve()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m but a dataset can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt overwrite itself.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1495\u001b[0m         )\n\u001b[1;32m   1497\u001b[0m fs\u001b[38;5;241m.\u001b[39mmakedirs(dataset_path, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;66;03m# Get json serializable state\u001b[39;00m\n",
      "\u001b[0;31mPermissionError\u001b[0m: Tried to overwrite /fs01/projects/cft_vlm/datasets/open_pmc/data/dataset/test but a dataset can't overwrite itself."
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "\n",
    "i = 0\n",
    "def dummy(example):\n",
    "  example['i'] = 0\n",
    "  return example\n",
    "  \n",
    "ds2 = ds.map(dummy, num_proc=24).remove_columns([\"i\"])\n",
    "shutil.rmtree(\"/projects/cft_vlm/datasets/open_pmc/data/dataset/test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6a7fe6a00c44bff8db9b1bf29ad5c5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/8 shards):   0%|          | 0/217980 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'/projects/cft_vlm/datasets/open_pmc/data/dataset/test'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds2.save_to_disk(\"/projects/cft_vlm/datasets/open_pmc/data/dataset/test_tmp\")\n",
    "shutil.move(\"/projects/cft_vlm/datasets/open_pmc/data/dataset/test_tmp\", \"/projects/cft_vlm/datasets/open_pmc/data/dataset/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data-00000-of-00008.arrow  data-00004-of-00008.arrow  dataset_info.json\n",
      "data-00001-of-00008.arrow  data-00005-of-00008.arrow  state.json\n",
      "data-00002-of-00008.arrow  data-00006-of-00008.arrow\n",
      "data-00003-of-00008.arrow  data-00007-of-00008.arrow\n"
     ]
    }
   ],
   "source": [
    "!ls /projects/cft_vlm/datasets/open_pmc/data/dataset/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qwenvl.data.input_processor import InputProcessor\n",
    "\n",
    "\n",
    "ip = InputProcessor(processor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_output(out, processor):\n",
    "  output_ids = out.logits.argmax(dim=-1)\n",
    "  output_texts = processor.batch_decode(output_ids, skip_special_tokens=False)\n",
    "  return output_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' '"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.decode([220,  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' b'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.decode([293,   ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "print() takes at most 4 keyword arguments (11 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mqwenvl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01margument\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ProcessingArguments\n\u001b[1;32m      3\u001b[0m args \u001b[38;5;241m=\u001b[39m ProcessingArguments()\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__dict__\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: print() takes at most 4 keyword arguments (11 given)"
     ]
    }
   ],
   "source": [
    "from qwenvl.argument import ProcessingArguments\n",
    "\n",
    "args = ProcessingArguments()\n",
    "print(**args.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|im_end|><|im_end|>1服务中心 to multi AI<|im_end|><|im_end|>\\n<|im_start|>\\n<|im_end|>I<|im_end|> process<|im_end|><|im_end|>\\n<|im_start|>\\n:I is is a<|im_end|>\\n<|endoftext|>user<|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|>\\n<|im_start|>\\n:I video is a person riding a..<|im_end|>\\n<|im_start|>user:This']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "vid_dir = Path('/projects/cft_vlm/datasets/openbiomedvid/data/vid_processed')\n",
    "\n",
    "conversations = [[\n",
    "  {\"role\": \"user\",\n",
    "   \"content\": [\n",
    "    {'text': \"Describe the video.\\n\"},\n",
    "    # {'video': vid_dir / '00cD-55fnaw_2590_3016.mp4'}\n",
    "    ]},\n",
    "  {\"role\": \"assistant\",\n",
    "   \"content\": [\n",
    "    {'text': \"This video describes.\\n\"}\n",
    "    ]}],\n",
    "  [{\"role\": \"user\",\n",
    "   \"content\": [\n",
    "     {'text': \"what is in this video?\"},\n",
    "    #  {'video': vid_dir / '01KY3GQKBzg_14_158.mp4'}\n",
    "   ]},\n",
    "  {\"role\": \"assistant\",\n",
    "   \"content\": [{'text': \"this video shows a person doing something interesting.\\n\"}]\n",
    "  }\n",
    "]]\n",
    "texts = processor.apply_chat_template(conversations, add_generation_prompt=True)\n",
    "texts_2 = processor.apply_chat_template([m for c in conversations for m in c], add_generation_prompt=True)\n",
    "texts_lengths = [len(processor.tokenizer.encode(t)) for t in texts]\n",
    "feat = processor(text=texts_2, return_tensors='pt', padding=True)\n",
    "attn_mask = torch.zeros_like(feat.input_ids)\n",
    "attn_mask[0, :texts_lengths[0]] = 1\n",
    "attn_mask[1, -texts_lengths[1]:] = 1\n",
    "feat.attention_mask = attn_mask\n",
    "out = model(**feat)\n",
    "decode_output(out, processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat.attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|im_end|><|im_end|>1服务中心 to multi AI<|im_end|><|im_end|>\\n<|im_start|>\\n<|im_end|>I<|im_end|> process<|im_end|><|im_end|>\\n<|im_start|>\\n:I is is a<|im_end|>\\n<|endoftext|>user<|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|>\\n<|im_start|>\\n:I video is a person riding a..<|im_end|>\\n<|im_start|>user:This']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_2 = processor.apply_chat_template([m for c in conversations for m in c], add_generation_prompt=True)\n",
    "feat_2 = processor(text=texts_2, return_tensors='pt', padding=True)\n",
    "out_2 = model(**feat_2)\n",
    "decode_output(out_2, processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_2.attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat.attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_di"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Literal\n",
    "\n",
    "\n",
    "def pad_and_stack_tensors(\n",
    "    tensors: List[torch.Tensor],\n",
    "    target_length: int,\n",
    "    padding_value: int = 0,\n",
    "    padding_side: Literal[\"right\", \"left\"] = \"right\",\n",
    "    truncation_side: Literal[\"right\", \"left\"] = \"right\",\n",
    "    **kwargs,\n",
    ") -> torch.Tensor:\n",
    "  \"\"\"\n",
    "  Pads or truncates a list of 1D tensors to a target length and stacks them.\n",
    "\n",
    "  This function processes a list of 1D tensors to ensure they all have the\n",
    "  same specified `target_length`. Tensors shorter than the target are padded,\n",
    "  and tensors longer than the target are truncated. The processed tensors are\n",
    "  then stacked into a single 2D tensor.\n",
    "\n",
    "  Args:\n",
    "      tensors (List[torch.Tensor]):\n",
    "          A list of 1D PyTorch tensors to process.\n",
    "      target_length (int):\n",
    "          The desired final length for each tensor.\n",
    "      padding_value (int, optional):\n",
    "          The value to use for padding. Defaults to 0.\n",
    "      padding_side (Literal[\"right\", \"left\"], optional):\n",
    "          The side to add padding to if a tensor is shorter than\n",
    "          `target_length`. Defaults to \"right\".\n",
    "      truncation_side (Literal[\"right\", \"left\"], optional):\n",
    "          The side to truncate from if a tensor is longer than\n",
    "          `target_length`. Defaults to \"right\".\n",
    "\n",
    "  Returns:\n",
    "      torch.Tensor:\n",
    "          A 2D tensor of shape (len(tensors), target_length) containing the\n",
    "          processed and stacked tensors. If the input list is empty, an empty\n",
    "          tensor of shape (0, target_length) is returned.\n",
    "\n",
    "  Raises:\n",
    "      ValueError: If `padding_side` or `truncation_side` are not \"left\" or \"right\".\n",
    "  \"\"\"\n",
    "  # --- 1. Input Validation ---\n",
    "  if padding_side not in [\"right\", \"left\"]:\n",
    "    raise ValueError(\n",
    "        f\"padding_side must be 'right' or 'left', but got '{padding_side}'\")\n",
    "  if truncation_side not in [\"right\", \"left\"]:\n",
    "    raise ValueError(\n",
    "        f\"truncation_side must be 'right' or 'left', but got '{truncation_side}'\")\n",
    "\n",
    "  # --- 2. Handle Edge Case: Empty Input List ---\n",
    "  if not tensors:\n",
    "    # Return an empty tensor with the correct shape.\n",
    "    # Assuming torch.long for token IDs, but could be adapted.\n",
    "    return torch.empty((0, target_length), dtype=torch.long)\n",
    "  attention_masks = [] # <-- ADDED\n",
    "  processed_tensors = []\n",
    "  # Use the device of the first tensor for consistency\n",
    "  device = tensors[0].device\n",
    "\n",
    "  # --- 3. Process each tensor ---\n",
    "  for tensor in tensors:\n",
    "    current_len = tensor.size(0)\n",
    "\n",
    "    # Ensure tensor is 1D\n",
    "    if tensor.dim() != 1:\n",
    "      raise ValueError(\n",
    "          f\"All tensors in the input list must be 1D, but found a tensor with shape {tensor.shape}\")\n",
    "\n",
    "    if current_len >= target_length:\n",
    "      if truncation_side == 'right':\n",
    "        processed_tensor = tensor[:target_length]\n",
    "      else:\n",
    "        processed_tensor = tensor[-target_length:]\n",
    "      mask = torch.ones(target_length, dtype=torch.long, device=device)\n",
    "    \n",
    "    elif current_len < target_length:\n",
    "      pad_len = target_length - current_len\n",
    "      if padding_side == 'right':\n",
    "        padding = (0, pad_len)\n",
    "      else:\n",
    "        padding = (pad_len, 0)\n",
    "    \n",
    "      processed_tensor = F.pad(tensor, padding, \"constant\", padding_value)\n",
    "      mask = torch.ones(current_len, dtype=torch.long, device=device)\n",
    "      mask = F.pad(mask, padding, \"constant\", 0)\n",
    "        \n",
    "    processed_tensors.append(processed_tensor)\n",
    "    attention_masks.append(mask)\n",
    "    \n",
    "  stacked_tensors = torch.stack(processed_tensors)\n",
    "  stacked_masks = torch.stack(attention_masks)\n",
    "\n",
    "  return stacked_tensors, stacked_masks # <-- MODIFIED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels\n",
      "---- torch.Size([1, 26878]) torch.Size([1, 26878]) True\n",
      "input_ids\n",
      "---- torch.Size([1, 26878]) torch.Size([1, 26878]) True\n",
      "attention_mask\n",
      "---- torch.Size([1, 26878]) torch.Size([1, 26878]) True\n"
     ]
    }
   ],
   "source": [
    "for k in 'labels', 'input_ids', 'attention_mask':\n",
    "  print(k)\n",
    "  print('----', features[k].shape, data_dict[k].shape, torch.equal(features[k], data_dict[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_grid_thw\n",
      "     False False\n",
      "video_grid_thw\n",
      "     torch.Size([2, 3]) torch.Size([2, 3]) True\n",
      "second_per_grid_ts\n",
      "     torch.Size([2]) torch.Size([2]) True\n",
      "pixel_values_videos\n",
      "     torch.Size([107280, 1176]) torch.Size([107280, 1176]) True\n",
      "pixel_values_images\n",
      "     False False\n"
     ]
    }
   ],
   "source": [
    "for k in 'image_grid_thw', 'video_grid_thw', 'second_per_grid_ts', 'pixel_values_videos', 'pixel_values_images':\n",
    "  print(k)\n",
    "  fk = features.get(k, None)\n",
    "  dk = data_dict.get(k, None)\n",
    "  if fk is None or dk is None:\n",
    "    print('    ', fk is not None, dk is not None)\n",
    "  else:\n",
    "    print('    ', fk.shape if fk is not None else None, dk.shape if dk is not None else None, torch.equal(fk, dk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.0262, 2.0000])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict.second_per_grid_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
