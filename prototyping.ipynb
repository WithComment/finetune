{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.chdir('/projects/cft_vlm/finetune')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration, Qwen2_5_VLProcessor\n",
    "import torch\n",
    "\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-3B-Instruct\", use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qwenvl.data.input_processor import InputProcessor\n",
    "\n",
    "\n",
    "ip = InputProcessor(processor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-02 14:49:42,325 - qwenvl.data.base - INFO - Loading dataset withcomment/openbiomedvid from /projects/cft_vlm/datasets/openbiomedvid/data/dataset\n",
      "2025-07-02 14:49:42,339 - qwenvl.data.sft - INFO - self.need_num_content_tokens()=False\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50f22c03f38049efb6bb654bfac715b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Counting total tokens (num_proc=24):   0%|          | 0/44188 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword argument `text_only` is not a valid argument for this processor and will be ignored.\n",
      "Keyword argument `text_only` is not a valid argument for this processor and will be ignored.\n",
      "Keyword argument `text_only` is not a valid argument for this processor and will be ignored.\n",
      "Keyword argument `text_only` is not a valid argument for this processor and will be ignored.\n",
      "Keyword argument `text_only` is not a valid argument for this processor and will be ignored.\n",
      "Keyword argument `text_only` is not a valid argument for this processor and will be ignored.\n",
      "Keyword argument `text_only` is not a valid argument for this processor and will be ignored.\n",
      "Keyword argument `text_only` is not a valid argument for this processor and will be ignored.\n",
      "Keyword argument `text_only` is not a valid argument for this processor and will be ignored.\n",
      "Keyword argument `text_only` is not a valid argument for this processor and will be ignored.\n",
      "Keyword argument `text_only` is not a valid argument for this processor and will be ignored.\n",
      "Keyword argument `text_only` is not a valid argument for this processor and will be ignored.\n",
      "Keyword argument `text_only` is not a valid argument for this processor and will be ignored.\n",
      "Keyword argument `text_only` is not a valid argument for this processor and will be ignored.\n",
      "Keyword argument `text_only` is not a valid argument for this processor and will be ignored.\n",
      "Keyword argument `text_only` is not a valid argument for this processor and will be ignored.\n",
      "Keyword argument `text_only` is not a valid argument for this processor and will be ignored.\n",
      "Keyword argument `text_only` is not a valid argument for this processor and will be ignored.\n",
      "Keyword argument `text_only` is not a valid argument for this processor and will be ignored.\n",
      "Keyword argument `text_only` is not a valid argument for this processor and will be ignored.\n",
      "Keyword argument `text_only` is not a valid argument for this processor and will be ignored.\n",
      "Keyword argument `text_only` is not a valid argument for this processor and will be ignored.\n",
      "Keyword argument `text_only` is not a valid argument for this processor and will be ignored.\n",
      "Keyword argument `text_only` is not a valid argument for this processor and will be ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "559a1d46b6a340e0819a2ed01cc5c68a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtering too long items (num_proc=24):   0%|          | 0/44188 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-02 14:50:05,089 - qwenvl.data.sft - INFO - Found 2168 / 44188 (0.0491) items with more than 16384 tokens.\n",
      "2025-07-02 14:50:05,090 - qwenvl.data.sft - INFO - Dataset withcomment/openbiomedvid has 42020 0.9509 items after filtering.\n",
      "2025-07-02 14:50:05,105 - qwenvl.data.sft - INFO - Data packing is enabled.\n",
      "2025-07-02 14:50:05,591 - qwenvl.data.sft - INFO - Loading bins from pickle file /projects/cft_vlm/datasets/openbiomedvid/data/dataset/bins.pkl.\n",
      "2025-07-02 14:50:05,602 - qwenvl.data.sft - INFO - Packing dataset into 25880 bins.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword argument `text_only` is not a valid argument for this processor and will be ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-02 14:50:05,607 - qwenvl.data.sft - INFO - Example item: ['<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|video_pad|><|vision_end|><|im_end|>\\n<|im_start|>user\\nWhat specific condition is highlighted in the video related to the position of the patella?\\n<|im_end|>\\n<|im_start|>assistant\\nPatella alta\\n<|im_end|>\\n<|im_start|>user\\nWhat anatomical feature is primarily responsible for the high position of the patella in the video?\\n<|im_end|>\\n<|im_start|>assistant\\nElongated patellar tendon\\n<|im_end|>\\n<|im_start|>user\\nWhat is observed on the inferior articular surface of the patella in the video?\\n<|im_end|>\\n<|im_start|>assistant\\nThinning and minor irregularities\\n<|im_end|>\\n<|im_start|>user\\nWhat type of stress does the patella experience due to patella alta as shown in the video?\\n<|im_end|>\\n<|im_start|>assistant\\nLateral subluxation stress\\n<|im_end|>\\n']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<qwenvl.data.openbiomedvid.OpenbiomedvidDataset at 0x7fd1093776d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qwenvl.argument import DataArguments\n",
    "from qwenvl.data.openbiomedvid import OpenbiomedvidDataset\n",
    "\n",
    "OpenbiomedvidDataset('openbiomedvid', ip, ip.proc_args, DataArguments())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Same in ip2: image_token\n",
      "Same in ip2: video_token\n",
      "Same in ip2: image_token_id\n",
      "Same in ip2: video_token_id\n",
      "Same in ip2: vs_id\n",
      "Same in ip2: ve_id\n",
      "Same in ip2: sys_prompt\n",
      "Same in ip2: mode\n",
      "Same in ip2: add_generation_prompt\n",
      "Same in ip2: add_vision_id\n",
      "Same in ip2: ignore_idx\n",
      "Same in ip2: proc_args\n",
      "Same in ip2: chat_template\n",
      "Different in ip2: image_processor\n",
      "Different in ip2: tokenizer\n",
      "Different in ip2: video_processor\n"
     ]
    }
   ],
   "source": [
    "for k, v in ip.__dict__.items():\n",
    "  if k not in ip2.__dict__:\n",
    "    print(f\"Missing in ip2: {k}\")\n",
    "  else:\n",
    "    if v != ip2.__dict__[k]:\n",
    "      print(f\"Different in ip2: {k}\")\n",
    "    else:\n",
    "      print(f\"Same in ip2: {k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip.save_pretrained(\"test_processor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'make_model_input' from 'qwenvl.data.utils' (/fs01/projects/cft_vlm/finetune/qwenvl/data/utils.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mqwenvl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01margument\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ProcessingArguments\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mqwenvl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m make_model_input\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[1;32m      4\u001b[0m vid_dir \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/projects/cft_vlm/datasets/openbiomedvid/data/vid_processed\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/fs01/projects/cft_vlm/finetune/qwenvl/data/__init__.py:7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseDataset\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msft\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SFTDataset\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mopenbiomedvid\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenbiomedvidDataset\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mopenpmc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenpmcDataset\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mopenpmc_tiny\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenpmcTinyDataset\n",
      "File \u001b[0;32m/fs01/projects/cft_vlm/finetune/qwenvl/data/openbiomedvid.py:14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mqwenvl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01margument\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ProcessingArguments\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mqwenvl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseDataset\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mqwenvl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m make_cot, make_model_input, smart_resize, verify_video\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msft\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SFTDataset\n\u001b[1;32m     20\u001b[0m VID_PROMPTS \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease describe the biomedical content shown in this video.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat medical or clinical content can you observe in this video?\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat medical or clinical aspects are being demonstrated here?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     41\u001b[0m )\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'make_model_input' from 'qwenvl.data.utils' (/fs01/projects/cft_vlm/finetune/qwenvl/data/utils.py)"
     ]
    }
   ],
   "source": [
    "from qwenvl.argument import ProcessingArguments\n",
    "from qwenvl.data.utils import make_model_input\n",
    "from pathlib import Path\n",
    "vid_dir = Path('/projects/cft_vlm/datasets/openbiomedvid/data/vid_processed')\n",
    "\n",
    "conversations = [[\n",
    "  {\"role\": \"user\",\n",
    "   \"content\": [\n",
    "    {'text': \"Describe the video.\\n\"},\n",
    "    {'video': vid_dir / '00cD-55fnaw_2590_3016.mp4'}\n",
    "    ]},\n",
    "  {\"role\": \"assistant\",\n",
    "   \"content\": [\n",
    "    {'text': \"This video describes.\\n\"}\n",
    "    ]}],\n",
    "  [{\"role\": \"user\",\n",
    "   \"content\": [\n",
    "     {'text': \"what is in this video?\"},\n",
    "     {'video': vid_dir / '01KY3GQKBzg_14_158.mp4'}\n",
    "   ]},\n",
    "  {\"role\": \"assistant\",\n",
    "   \"content\": [{'text': \"this video shows a person doing something interesting.\\n\"}]\n",
    "  }\n",
    "]]\n",
    "proc_args = ProcessingArguments()\n",
    "data_dict, text0 = make_model_input([conversations], processor, proc_args, for_training=True, mode='ift')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_di"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Literal\n",
    "\n",
    "\n",
    "def pad_and_stack_tensors(\n",
    "    tensors: List[torch.Tensor],\n",
    "    target_length: int,\n",
    "    padding_value: int = 0,\n",
    "    padding_side: Literal[\"right\", \"left\"] = \"right\",\n",
    "    truncation_side: Literal[\"right\", \"left\"] = \"right\",\n",
    "    **kwargs,\n",
    ") -> torch.Tensor:\n",
    "  \"\"\"\n",
    "  Pads or truncates a list of 1D tensors to a target length and stacks them.\n",
    "\n",
    "  This function processes a list of 1D tensors to ensure they all have the\n",
    "  same specified `target_length`. Tensors shorter than the target are padded,\n",
    "  and tensors longer than the target are truncated. The processed tensors are\n",
    "  then stacked into a single 2D tensor.\n",
    "\n",
    "  Args:\n",
    "      tensors (List[torch.Tensor]):\n",
    "          A list of 1D PyTorch tensors to process.\n",
    "      target_length (int):\n",
    "          The desired final length for each tensor.\n",
    "      padding_value (int, optional):\n",
    "          The value to use for padding. Defaults to 0.\n",
    "      padding_side (Literal[\"right\", \"left\"], optional):\n",
    "          The side to add padding to if a tensor is shorter than\n",
    "          `target_length`. Defaults to \"right\".\n",
    "      truncation_side (Literal[\"right\", \"left\"], optional):\n",
    "          The side to truncate from if a tensor is longer than\n",
    "          `target_length`. Defaults to \"right\".\n",
    "\n",
    "  Returns:\n",
    "      torch.Tensor:\n",
    "          A 2D tensor of shape (len(tensors), target_length) containing the\n",
    "          processed and stacked tensors. If the input list is empty, an empty\n",
    "          tensor of shape (0, target_length) is returned.\n",
    "\n",
    "  Raises:\n",
    "      ValueError: If `padding_side` or `truncation_side` are not \"left\" or \"right\".\n",
    "  \"\"\"\n",
    "  # --- 1. Input Validation ---\n",
    "  if padding_side not in [\"right\", \"left\"]:\n",
    "    raise ValueError(\n",
    "        f\"padding_side must be 'right' or 'left', but got '{padding_side}'\")\n",
    "  if truncation_side not in [\"right\", \"left\"]:\n",
    "    raise ValueError(\n",
    "        f\"truncation_side must be 'right' or 'left', but got '{truncation_side}'\")\n",
    "\n",
    "  # --- 2. Handle Edge Case: Empty Input List ---\n",
    "  if not tensors:\n",
    "    # Return an empty tensor with the correct shape.\n",
    "    # Assuming torch.long for token IDs, but could be adapted.\n",
    "    return torch.empty((0, target_length), dtype=torch.long)\n",
    "  attention_masks = [] # <-- ADDED\n",
    "  processed_tensors = []\n",
    "  # Use the device of the first tensor for consistency\n",
    "  device = tensors[0].device\n",
    "\n",
    "  # --- 3. Process each tensor ---\n",
    "  for tensor in tensors:\n",
    "    current_len = tensor.size(0)\n",
    "\n",
    "    # Ensure tensor is 1D\n",
    "    if tensor.dim() != 1:\n",
    "      raise ValueError(\n",
    "          f\"All tensors in the input list must be 1D, but found a tensor with shape {tensor.shape}\")\n",
    "\n",
    "    if current_len >= target_length:\n",
    "      if truncation_side == 'right':\n",
    "        processed_tensor = tensor[:target_length]\n",
    "      else:\n",
    "        processed_tensor = tensor[-target_length:]\n",
    "      mask = torch.ones(target_length, dtype=torch.long, device=device)\n",
    "    \n",
    "    elif current_len < target_length:\n",
    "      pad_len = target_length - current_len\n",
    "      if padding_side == 'right':\n",
    "        padding = (0, pad_len)\n",
    "      else:\n",
    "        padding = (pad_len, 0)\n",
    "    \n",
    "      processed_tensor = F.pad(tensor, padding, \"constant\", padding_value)\n",
    "      mask = torch.ones(current_len, dtype=torch.long, device=device)\n",
    "      mask = F.pad(mask, padding, \"constant\", 0)\n",
    "        \n",
    "    processed_tensors.append(processed_tensor)\n",
    "    attention_masks.append(mask)\n",
    "    \n",
    "  stacked_tensors = torch.stack(processed_tensors)\n",
    "  stacked_masks = torch.stack(attention_masks)\n",
    "\n",
    "  return stacked_tensors, stacked_masks # <-- MODIFIED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels\n",
      "---- torch.Size([1, 26878]) torch.Size([1, 26878]) True\n",
      "input_ids\n",
      "---- torch.Size([1, 26878]) torch.Size([1, 26878]) True\n",
      "attention_mask\n",
      "---- torch.Size([1, 26878]) torch.Size([1, 26878]) True\n"
     ]
    }
   ],
   "source": [
    "for k in 'labels', 'input_ids', 'attention_mask':\n",
    "  print(k)\n",
    "  print('----', features[k].shape, data_dict[k].shape, torch.equal(features[k], data_dict[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_grid_thw\n",
      "     False False\n",
      "video_grid_thw\n",
      "     torch.Size([2, 3]) torch.Size([2, 3]) True\n",
      "second_per_grid_ts\n",
      "     torch.Size([2]) torch.Size([2]) True\n",
      "pixel_values_videos\n",
      "     torch.Size([107280, 1176]) torch.Size([107280, 1176]) True\n",
      "pixel_values_images\n",
      "     False False\n"
     ]
    }
   ],
   "source": [
    "for k in 'image_grid_thw', 'video_grid_thw', 'second_per_grid_ts', 'pixel_values_videos', 'pixel_values_images':\n",
    "  print(k)\n",
    "  fk = features.get(k, None)\n",
    "  dk = data_dict.get(k, None)\n",
    "  if fk is None or dk is None:\n",
    "    print('    ', fk is not None, dk is not None)\n",
    "  else:\n",
    "    print('    ', fk.shape if fk is not None else None, dk.shape if dk is not None else None, torch.equal(fk, dk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.0262, 2.0000])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict.second_per_grid_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
