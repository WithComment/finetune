{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.chdir('/projects/cft_vlm/finetune')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d754e6f88af47dd8b0953eee3e0aa53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-3B-Instruct\", use_fast=True)\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "  \"Qwen/Qwen2.5-VL-3B-Instruct\",\n",
    "  torch_dtype=torch.bfloat16,\n",
    "  device_map='auto',\n",
    "  attn_implementation='flash_attention_2'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "What is this logo for?<|vision_start|><|image_pad|><|vision_end|><|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 47])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qwenvl.data.utils import make_model_input\n",
    "\n",
    "conversation = [\n",
    "  {\"role\": \"user\",\n",
    "   \"content\": [\n",
    "    {\n",
    "      'type': 'text',\n",
    "      'text': \"What is this logo for?\"\n",
    "    },\n",
    "    {\n",
    "      'type': 'image',\n",
    "      'image': 'python_logo.png'\n",
    "    }\n",
    "    ]}\n",
    "]\n",
    "processor.image_processor.max_pixels = 20000\n",
    "text = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)\n",
    "print(text)\n",
    "inputs = make_model_input(conversation, processor, None, False)\n",
    "inputs['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...existing code...\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers.modeling_outputs import BaseModelOutputWithPast\n",
    "\n",
    "def decode_tokens(tokenizer, tensor):\n",
    "  if tensor is None:\n",
    "    return None\n",
    "  if tensor.ndim > 1:\n",
    "    tensor = tensor[0]\n",
    "  return tokenizer.decode(tensor.tolist(), skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# Prepare storage for hooks\n",
    "record = {}\n",
    "\n",
    "def get_output_hook(name):\n",
    "  def hook(module, input, output):\n",
    "    if isinstance(output, BaseModelOutputWithPast):\n",
    "      output = output['last_hidden_state']\n",
    "      \n",
    "    record[f\"{name}_output\"] = output.detach().cpu()\n",
    "  return hook\n",
    "\n",
    "\n",
    "# Attach hooks\n",
    "lang_hook = model.language_model.register_forward_hook(\n",
    "    get_output_hook(\"language_model\")\n",
    ")\n",
    "vis_hook = model.visual.register_forward_hook(\n",
    "    get_output_hook(\"visual\")\n",
    ")\n",
    "\n",
    "# Run forward pass\n",
    "try:\n",
    "  with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "except Exception as e:\n",
    "  raise e\n",
    "finally:\n",
    "  # Remove hooks\n",
    "  lang_hook.remove()\n",
    "  vis_hook.remove()\n",
    "\n",
    "# Get logits and decode output tokens\n",
    "logits = outputs.logits\n",
    "output_ids = logits.argmax(-1)\n",
    "decoded_output = decode_tokens(processor.tokenizer, output_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 47, 151936])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs['logits'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_pad_token = processor.image_token_id\n",
    "img_token_locs = torch.where(inputs['input_ids'] == image_pad_token)\n",
    "img_token_locs = (img_token_locs[0][:-1], img_token_locs[1][:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the image tokens from the inputs\n",
    "img_predict = record['language_model_output'][img_token_locs]\n",
    "img_targets = record['visual_output'][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06591796875"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.cosine_similarity(\n",
    "    img_predict, img_targets, dim=-1\n",
    ").mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_predict = record['language_model_output']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cft_vlm",
   "language": "python",
   "name": "cft_vlm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
