{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.chdir('/projects/cft_vlm/finetune')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dccc9287b334474b865d608371bcf08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-3B-Instruct\", use_fast=True)\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "  \"Qwen/Qwen2.5-VL-3B-Instruct\",\n",
    "  torch_dtype=torch.bfloat16,\n",
    "  device_map='auto',\n",
    "  attn_implementation='flash_attention_2'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "560a787951754fa1b99186f28f6a104b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/350 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08919af4616d4b749549465d96de9044",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "046ceea8aedc4b749fa160a6c6c5984a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'jpg': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=210x363>,\n",
       " 'jsonl': b'{\"PMC_ID\": \"PMC10060508\", \"image\": \"test-0000-000000.jpg\", \"sub_caption\": \"Postoperative MRI indicates no spinal cord compression and no spinal canal occupancy.\", \"full_caption\": \"Typical preoperative, postoperative, and follow-up images of severe thoracolumbar burst fractures with posterior midline spinal approach for spinal canal limited decompression and 13-mm titanium mesh implantation (case #2). (A,B) True lateral digital x-rays of the patient\\'s thoracolumbar spine before the operation showing the L1 fracture. (C,D) True lateral digital x-rays of the thoracolumbar vertebrae after the operation showing good fixation and recovery of the height of the anterior edge of the L1 vertebra. (E,F) Preoperative magnetic resonance imaging (MRI) showing L1 burst fracture, spinal cord compression, and obvious spinal canal occupancy. (G\\\\u2013J) Postoperative MRI indicates no spinal cord compression and no spinal canal occupancy. (I)\\\\u00a0Postoperative computed tomography (CT) indicates a good position of the titanium mesh, screws, and titanium rods.\", \"intext_refs_summary\": \"Radiological imaging results 1 year post-surgery demonstrate successful fusion of the bone graft within the 13-mm titanium mesh across all patients. The imaging indicates the absence of space-occupying lesions in the spinal canal of the injured vertebrae, as well as no evidence of loosening or detachment of screws and connecting rods during the follow-up period.\", \"intext_refs\": \"The radiological data 1 year after surgery showed fusion of the bone graft in the 13-mm titanium mesh in all patients. There were no space-occupying lesions in the spinal canal of the injured vertebrae and no signs of loosening and falling off of screws and connecting rods during follow-up. The imaging results of typical cases are shown inFigures\\\\u00a01,2.\", \"modality\": \"R\"}\\n',\n",
       " '__key__': 'test-0000-000000',\n",
       " '__url__': '/scratch/ssd004/scratch/xiaowenz/.cache/huggingface/hub/datasets--vector-institute--open-pmc/snapshots/a7add5e1b0d8659281931898b1377d27c0d1a2dd/test-0000.tar'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "ds = datasets.load_dataset('vector-institute/open-pmc', split='test')\n",
    "ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Describe the video.\n",
      "Video 1: <|vision_start|><|video_pad|><|vision_end|><|im_end|>\n",
      "<|im_start|>user\n",
      "Describe the video.\n",
      "Video 2: <|vision_start|><|video_pad|><|vision_end|><|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused or unrecognized kwargs: fps, return_tensors.\n"
     ]
    }
   ],
   "source": [
    "from qwenvl.argument import ProcessingArguments\n",
    "from qwenvl.data.utils import make_model_input\n",
    "from pathlib import Path\n",
    "\n",
    "vid_dir = Path('/projects/cft_vlm/datasets/openbiomedvid/data/vid_segments')\n",
    "\n",
    "conversation = [\n",
    "  {\"role\": \"user\",\n",
    "   \"content\": [\n",
    "    {\n",
    "      'type': 'text',\n",
    "      'text': \"Describe the video.\\n\"\n",
    "    },\n",
    "    {\n",
    "      'type': 'video',\n",
    "      'video': str(vid_dir / '00cD-55fnaw_2590_3016.mp4')\n",
    "    }\n",
    "    ]},\n",
    "  {\"role\": \"user\",\n",
    "   \"content\": [\n",
    "    {\n",
    "      'type': 'text',\n",
    "      'text': \"Describe the video.\\n\"\n",
    "    },\n",
    "    {\n",
    "      'type': 'video',\n",
    "      'video': str(vid_dir / '01KY3GQKBzg_14_158.mp4')\n",
    "    }\n",
    "    ]},\n",
    "]\n",
    "proc_args = ProcessingArguments()\n",
    "\n",
    "text = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False, add_vision_id=True)\n",
    "print(text)\n",
    "inputs = make_model_input(conversation, processor, proc_args, for_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs01/projects/cft_vlm/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:2479: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['system\\nYou are a helpful assistant.\\nuser\\nDescribe the video.\\nVideo 1: \\nuser\\nDescribe the video.\\nVideo 2: \\nassistant\\nThe video begins with a title card that reads \"Nick Mamalis, M.D.\" and \"Professor - Department of Ophthalmology/Visual Sciences.\" The card']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "  gen_config = model.generation_config\n",
    "  gen_config.max_new_tokens = 32\n",
    "  \n",
    "  outputs = model.generate(\n",
    "    **inputs,\n",
    "    generation_config=gen_config,\n",
    "  )\n",
    "\n",
    "processor.batch_decode(outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[151644,   8948,    198,  ..., 151653, 151645,    198]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]]), 'pixel_values_videos': tensor([[ 1.8865,  1.8865,  1.8865,  ...,  2.0890,  2.0890,  2.0890],\n",
       "        [ 1.8865,  1.8865,  1.8865,  ...,  2.0890,  2.0890,  2.0890],\n",
       "        [ 1.8281,  1.8281,  1.8281,  ...,  2.0464,  2.0464,  2.0464],\n",
       "        ...,\n",
       "        [-1.6171, -1.6171, -1.6171,  ..., -1.2385, -1.2385, -1.2385],\n",
       "        [-1.5587, -1.5587, -1.5733,  ..., -1.2385, -1.2385, -1.2385],\n",
       "        [-1.6609, -1.6609, -1.6609,  ..., -1.2527, -1.2527, -1.2527]]), 'video_grid_thw': tensor([[15, 26, 46],\n",
       "        [15, 26, 46]]), 'second_per_grid_ts': tensor([28.3667,  9.5667]), 'labels': tensor([[-100, -100, -100,  ..., -100, -100, -100]])}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...existing code...\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers.modeling_outputs import BaseModelOutputWithPast\n",
    "\n",
    "def decode_tokens(tokenizer, tensor):\n",
    "  if tensor is None:\n",
    "    return None\n",
    "  if tensor.ndim > 1:\n",
    "    tensor = tensor[0]\n",
    "  return tokenizer.decode(tensor.tolist(), skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# Prepare storage for hooks\n",
    "record = {}\n",
    "\n",
    "def get_output_hook(name):\n",
    "  def hook(module, input, output):\n",
    "    if isinstance(output, BaseModelOutputWithPast):\n",
    "      output = output['last_hidden_state']\n",
    "      \n",
    "    record[f\"{name}_output\"] = output.detach().cpu()\n",
    "  return hook\n",
    "\n",
    "\n",
    "# Attach hooks\n",
    "lang_hook = model.language_model.register_forward_hook(\n",
    "    get_output_hook(\"language_model\")\n",
    ")\n",
    "vis_hook = model.visual.register_forward_hook(\n",
    "    get_output_hook(\"visual\")\n",
    ")\n",
    "\n",
    "# Run forward pass\n",
    "try:\n",
    "  with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "except Exception as e:\n",
    "  raise e\n",
    "finally:\n",
    "  # Remove hooks\n",
    "  lang_hook.remove()\n",
    "  vis_hook.remove()\n",
    "\n",
    "# Get logits and decode output tokens\n",
    "logits = outputs.logits\n",
    "output_ids = logits.argmax(-1)\n",
    "decoded_output = decode_tokens(processor.tokenizer, output_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 47, 151936])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs['logits'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_pad_token = processor.image_token_id\n",
    "img_token_locs = torch.where(inputs['input_ids'] == image_pad_token)\n",
    "img_token_locs = (img_token_locs[0][:-1], img_token_locs[1][:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the image tokens from the inputs\n",
    "img_predict = record['language_model_output'][img_token_locs]\n",
    "img_targets = record['visual_output'][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06591796875"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.cosine_similarity(\n",
    "    img_predict, img_targets, dim=-1\n",
    ").mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_predict = record['language_model_output']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cft_vlm",
   "language": "python",
   "name": "cft_vlm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
