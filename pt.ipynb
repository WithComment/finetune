{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.chdir('/projects/cft_vlm/finetune')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "from qwenvl.train.argument import *\n",
    "import torch\n",
    "\n",
    "\n",
    "def set_processor(processor_args, processor):\n",
    "  tokenizer = processor.tokenizer\n",
    "  img_processor = processor.image_processor\n",
    "  vid_processor = processor.video_processor\n",
    "  \n",
    "  tokenizer.padding_side = processor_args.padding_side\n",
    "  tokenizer.model_max_length = processor_args.model_max_length\n",
    "  img_processor.max_pixels = processor_args.image_max_pixels\n",
    "  img_processor.min_pixels = processor_args.image_min_pixels\n",
    "  img_processor.size[\"shortest_edge\"] = processor_args.shortest_edge\n",
    "  vid_processor.max_frame_pixels = processor_args.video_max_pixels\n",
    "  vid_processor.min_frame_pixels = processor_args.video_min_pixels\n",
    "  vid_processor.size['shortest_edge'] = processor_args.shortest_edge\n",
    "  vid_processor.default_to_square = processor_args.video_default_to_square\n",
    "  \n",
    "  return copy.deepcopy(processor)\n",
    "\n",
    "\n",
    "model_args, data_args, training_args, proc_args = ModelArguments(), DataArguments(), TrainingArguments(), ProcessingArguments()\n",
    "\n",
    "processor = transformers.AutoProcessor.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    use_fast=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2VLVideoProcessor {\n",
       "  \"_valid_kwargs_names\": [\n",
       "    \"do_convert_rgb\",\n",
       "    \"do_resize\",\n",
       "    \"size\",\n",
       "    \"size_divisor\",\n",
       "    \"default_to_square\",\n",
       "    \"resample\",\n",
       "    \"do_rescale\",\n",
       "    \"rescale_factor\",\n",
       "    \"do_normalize\",\n",
       "    \"image_mean\",\n",
       "    \"image_std\",\n",
       "    \"do_pad\",\n",
       "    \"do_center_crop\",\n",
       "    \"crop_size\",\n",
       "    \"data_format\",\n",
       "    \"input_data_format\",\n",
       "    \"device\",\n",
       "    \"min_pixels\",\n",
       "    \"max_pixels\",\n",
       "    \"patch_size\",\n",
       "    \"temporal_patch_size\",\n",
       "    \"merge_size\"\n",
       "  ],\n",
       "  \"crop_size\": null,\n",
       "  \"data_format\": \"channels_first\",\n",
       "  \"default_to_square\": false,\n",
       "  \"device\": null,\n",
       "  \"do_center_crop\": null,\n",
       "  \"do_convert_rgb\": true,\n",
       "  \"do_normalize\": true,\n",
       "  \"do_pad\": null,\n",
       "  \"do_rescale\": true,\n",
       "  \"do_resize\": true,\n",
       "  \"image_mean\": [\n",
       "    0.48145466,\n",
       "    0.4578275,\n",
       "    0.40821073\n",
       "  ],\n",
       "  \"image_processor_type\": \"Qwen2VLImageProcessor\",\n",
       "  \"image_std\": [\n",
       "    0.26862954,\n",
       "    0.26130258,\n",
       "    0.27577711\n",
       "  ],\n",
       "  \"input_data_format\": null,\n",
       "  \"max_frame_pixels\": 150528,\n",
       "  \"max_pixels\": 12845056,\n",
       "  \"merge_size\": 2,\n",
       "  \"min_frame_pixels\": 50176,\n",
       "  \"min_pixels\": 3136,\n",
       "  \"model_valid_processing_keys\": [\n",
       "    \"do_convert_rgb\",\n",
       "    \"do_resize\",\n",
       "    \"size\",\n",
       "    \"size_divisor\",\n",
       "    \"default_to_square\",\n",
       "    \"resample\",\n",
       "    \"do_rescale\",\n",
       "    \"rescale_factor\",\n",
       "    \"do_normalize\",\n",
       "    \"image_mean\",\n",
       "    \"image_std\",\n",
       "    \"do_pad\",\n",
       "    \"do_center_crop\",\n",
       "    \"crop_size\",\n",
       "    \"data_format\",\n",
       "    \"input_data_format\",\n",
       "    \"device\",\n",
       "    \"min_pixels\",\n",
       "    \"max_pixels\",\n",
       "    \"patch_size\",\n",
       "    \"temporal_patch_size\",\n",
       "    \"merge_size\"\n",
       "  ],\n",
       "  \"patch_size\": 14,\n",
       "  \"processor_class\": \"Qwen2_5_VLProcessor\",\n",
       "  \"resample\": 3,\n",
       "  \"rescale_factor\": 0.00392156862745098,\n",
       "  \"size\": {\n",
       "    \"longest_edge\": 12845056,\n",
       "    \"shortest_edge\": 224\n",
       "  },\n",
       "  \"size_divisor\": null,\n",
       "  \"temporal_patch_size\": 2,\n",
       "  \"video_processor_type\": \"Qwen2VLVideoProcessor\"\n",
       "}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.video_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset /projects/cft_vlm/datasets/openpmc/data/dataset/validation already processed with the same processor and video args.\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "from qwenvl.data.data_qwen import make_supervised_data_module\n",
    "processor = set_processor(proc_args, processor)\n",
    "torch.set_num_threads(1)\n",
    "data_args.dataset_use = \"openpmc_validation\"\n",
    "data_module = make_supervised_data_module(processor=processor, data_args=data_args, proc_args=proc_args, num_proc=32)\n",
    "from qwenvl.train.trainer import Trainer\n",
    "\n",
    "\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration\n",
    "os.makedirs(training_args.output_dir, exist_ok=True)\n",
    "\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    cache_dir=training_args.cache_dir,\n",
    "    attn_implementation='eager',\n",
    "    torch_dtype=(torch.bfloat16 if training_args.bf16 else None),\n",
    ")\n",
    "\n",
    "if torch.distributed.get_rank() == 0:\n",
    "  model.visual.print_trainable_parameters()\n",
    "  model.model.print_trainable_parameters()\n",
    "  \n",
    "model.config.use_cache = False\n",
    "\n",
    "if training_args.gradient_checkpointing:\n",
    "  if hasattr(model, \"enable_input_require_grads\"):\n",
    "    model.enable_input_require_grads()\n",
    "  else:\n",
    "    def make_inputs_require_grad(module, input, output):\n",
    "      output.requires_grad_(True)\n",
    "    model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)\n",
    "trainer = Trainer(\n",
    "    model=model, processing_class=processor, args=training_args, **data_module\n",
    ")\n",
    "\n",
    "if list(pathlib.Path(training_args.output_dir).glob(\"checkpoint-*\")):\n",
    "  print(\"checkpoint found, resume training\")\n",
    "  trainer.train(resume_from_checkpoint=True)\n",
    "else:\n",
    "  trainer.train()\n",
    "  \n",
    "trainer.save_state()\n",
    "processor.save_pretrained(training_args.output_dir)\n",
    "\n",
    "model.config.use_cache = True\n",
    "\n",
    "safe_save_model_for_hf_trainer(\n",
    "    trainer=trainer, output_dir=training_args.output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cft_vlm",
   "language": "python",
   "name": "cft_vlm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
