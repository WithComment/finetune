{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.chdir('/projects/cft_vlm/finetune')\n",
    "import transformers\n",
    "from transformers import AutoProcessor\n",
    "import datasets\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-3B-Instruct\", use_fast=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7041c72416b844ea960652e62d4370f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c3fa58894404cdeac9fb924860adc7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import Qwen2_5_VLForConditionalGeneration\n",
    "\n",
    "og_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "  \"Qwen/Qwen2.5-VL-3B-Instruct\",\n",
    "  torch_dtype=\"auto\",\n",
    "  device_map=\"auto\", \n",
    "  attn_implementation=\"flash_attention_2\")\n",
    "\n",
    "checkpoint_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "  \"/projects/cft_vlm/.checkpoint/Qwen/Qwen2.5-VL-3B-Instruct-openbiomedvid/checkpoint-100\",\n",
    "  torch_dtype=\"auto\",\n",
    "  device_map=\"auto\",\n",
    "  attn_implementation=\"flash_attention_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter embed_tokens.weight differs between the original and checkpoint model.\n",
      "Parameter layers.0.self_attn.q_proj.weight differs between the original and checkpoint model.\n",
      "Parameter layers.0.self_attn.q_proj.bias differs between the original and checkpoint model.\n",
      "Parameter layers.0.self_attn.k_proj.weight differs between the original and checkpoint model.\n",
      "Parameter layers.0.self_attn.k_proj.bias is the same in both models.\n",
      "Parameter layers.0.self_attn.v_proj.weight differs between the original and checkpoint model.\n",
      "Parameter layers.0.self_attn.v_proj.bias differs between the original and checkpoint model.\n",
      "Parameter layers.0.self_attn.o_proj.weight differs between the original and checkpoint model.\n",
      "Parameter layers.0.mlp.gate_proj.weight differs between the original and checkpoint model.\n",
      "Parameter layers.0.mlp.up_proj.weight differs between the original and checkpoint model.\n",
      "Parameter layers.0.mlp.down_proj.weight differs between the original and checkpoint model.\n",
      "Parameter layers.0.input_layernorm.weight is the same in both models.\n",
      "Parameter layers.0.post_attention_layernorm.weight differs between the original and checkpoint model.\n",
      "Parameter layers.1.self_attn.q_proj.weight differs between the original and checkpoint model.\n",
      "Parameter layers.1.self_attn.q_proj.bias differs between the original and checkpoint model.\n",
      "Parameter layers.1.self_attn.k_proj.weight differs between the original and checkpoint model.\n",
      "Parameter layers.1.self_attn.k_proj.bias is the same in both models.\n",
      "Parameter layers.1.self_attn.v_proj.weight differs between the original and checkpoint model.\n",
      "Parameter layers.1.self_attn.v_proj.bias differs between the original and checkpoint model.\n",
      "Parameter layers.1.self_attn.o_proj.weight differs between the original and checkpoint model.\n",
      "Parameter layers.1.mlp.gate_proj.weight differs between the original and checkpoint model.\n",
      "Parameter layers.1.mlp.up_proj.weight differs between the original and checkpoint model.\n",
      "Parameter layers.1.mlp.down_proj.weight differs between the original and checkpoint model.\n",
      "Parameter layers.1.input_layernorm.weight differs between the original and checkpoint model.\n",
      "Parameter layers.1.post_attention_layernorm.weight differs between the original and checkpoint model.\n",
      "Parameter layers.2.self_attn.q_proj.weight differs between the original and checkpoint model.\n",
      "Parameter layers.2.self_attn.q_proj.bias differs between the original and checkpoint model.\n",
      "Parameter layers.2.self_attn.k_proj.weight differs between the original and checkpoint model.\n",
      "Parameter layers.2.self_attn.k_proj.bias is the same in both models.\n",
      "Parameter layers.2.self_attn.v_proj.weight differs between the original and checkpoint model.\n",
      "Parameter layers.2.self_attn.v_proj.bias differs between the original and checkpoint model.\n",
      "Parameter layers.2.self_attn.o_proj.weight differs between the original and checkpoint model.\n",
      "Parameter layers.2.mlp.gate_proj.weight differs between the original and checkpoint model.\n",
      "Parameter layers.2.mlp.up_proj.weight differs between the original and checkpoint model.\n",
      "Parameter layers.2.mlp.down_proj.weight differs between the original and checkpoint model.\n",
      "Parameter layers.2.input_layernorm.weight differs between the original and checkpoint model.\n",
      "Parameter layers.2.post_attention_layernorm.weight differs between the original and checkpoint model.\n",
      "Parameter layers.3.self_attn.q_proj.weight differs between the original and checkpoint model.\n",
      "Parameter layers.3.self_attn.q_proj.bias differs between the original and checkpoint model.\n",
      "Parameter layers.3.self_attn.k_proj.weight differs between the original and checkpoint model.\n",
      "Parameter layers.3.self_attn.k_proj.bias differs between the original and checkpoint model.\n",
      "Parameter layers.3.self_attn.v_proj.weight differs between the original and checkpoint model.\n",
      "Parameter layers.3.self_attn.v_proj.bias differs between the original and checkpoint model.\n",
      "Parameter layers.3.self_attn.o_proj.weight differs between the original and checkpoint model.\n",
      "Parameter layers.3.mlp.gate_proj.weight differs between the original and checkpoint model.\n",
      "Parameter layers.3.mlp.up_proj.weight differs between the original and checkpoint model.\n",
      "Parameter layers.3.mlp.down_proj.weight differs between the original and checkpoint model.\n",
      "Parameter layers.3.input_layernorm.weight differs between the original and checkpoint model.\n",
      "Parameter layers.3.post_attention_layernorm.weight differs between the original and checkpoint model.\n",
      "Parameter layers.4.self_attn.q_proj.weight differs between the original and checkpoint model.\n",
      "Parameter layers.4.self_attn.q_proj.bias differs between the original and checkpoint model.\n",
      "Parameter layers.4.self_attn.k_proj.weight differs between the original and checkpoint model.\n",
      "Parameter layers.4.self_attn.k_proj.bias is the same in both models.\n",
      "Parameter layers.4.self_attn.v_proj.weight differs between the original and checkpoint model.\n",
      "Parameter layers.4.self_attn.v_proj.bias differs between the original and checkpoint model.\n",
      "Parameter layers.4.self_attn.o_proj.weight differs between the original and checkpoint model.\n",
      "Parameter layers.4.mlp.gate_proj.weight differs between the original and checkpoint model.\n",
      "Parameter layers.4.mlp.up_proj.weight differs between the original and checkpoint model.\n",
      "Parameter layers.4.mlp.down_proj.weight differs between the original and checkpoint model.\n",
      "Parameter layers.4.input_layernorm.weight differs between the original and checkpoint model.\n",
      "Parameter layers.4.post_attention_layernorm.weight differs between the original and checkpoint model.\n",
      "Parameter layers.5.self_attn.q_proj.weight differs between the original and checkpoint model.\n",
      "Parameter layers.5.self_attn.q_proj.bias differs between the original and checkpoint model.\n",
      "Parameter layers.5.self_attn.k_proj.weight differs between the original and checkpoint model.\n",
      "Parameter layers.5.self_attn.k_proj.bias differs between the original and checkpoint model.\n",
      "Parameter layers.5.self_attn.v_proj.weight differs between the original and checkpoint model.\n",
      "Parameter layers.5.self_attn.v_proj.bias differs between the original and checkpoint model.\n",
      "Parameter layers.5.self_attn.o_proj.weight differs between the original and checkpoint model.\n",
      "Parameter layers.5.mlp.gate_proj.weight differs between the original and checkpoint model.\n",
      "Parameter layers.5.mlp.up_proj.weight differs between the original and checkpoint model.\n",
      "Parameter layers.5.mlp.down_proj.weight differs between the original and checkpoint model.\n",
      "Parameter layers.5.input_layernorm.weight differs between the original and checkpoint model.\n",
      "Parameter layers.5.post_attention_layernorm.weight differs between the original and checkpoint model.\n",
      "Parameter layers.6.self_attn.q_proj.weight differs between the original and checkpoint model.\n",
      "Parameter layers.6.self_attn.q_proj.bias differs between the original and checkpoint model.\n",
      "Parameter layers.6.self_attn.k_proj.weight differs between the original and checkpoint model.\n",
      "Parameter layers.6.self_attn.k_proj.bias is the same in both models.\n",
      "Parameter layers.6.self_attn.v_proj.weight differs between the original and checkpoint model.\n",
      "Parameter layers.6.self_attn.v_proj.bias differs between the original and checkpoint model.\n",
      "Parameter layers.6.self_attn.o_proj.weight differs between the original and checkpoint model.\n",
      "Parameter layers.6.mlp.gate_proj.weight differs between the original and checkpoint model.\n",
      "Parameter layers.6.mlp.up_proj.weight differs between the original and checkpoint model.\n",
      "Parameter layers.6.mlp.down_proj.weight differs between the original and checkpoint model.\n",
      "Parameter layers.6.input_layernorm.weight differs between the original and checkpoint model.\n",
      "Parameter layers.6.post_attention_layernorm.weight differs between the original and checkpoint model.\n",
      "Parameter layers.7.self_attn.q_proj.weight differs between the original and checkpoint model.\n",
      "Parameter layers.7.self_attn.q_proj.bias differs between the original and checkpoint model.\n",
      "Parameter layers.7.self_attn.k_proj.weight differs between the original and checkpoint model.\n",
      "Parameter layers.7.self_attn.k_proj.bias differs between the original and checkpoint model.\n",
      "Parameter layers.7.self_attn.v_proj.weight differs between the original and checkpoint model.\n",
      "Parameter layers.7.self_attn.v_proj.bias differs between the original and checkpoint model.\n",
      "Parameter layers.7.self_attn.o_proj.weight differs between the original and checkpoint model.\n",
      "Parameter layers.7.mlp.gate_proj.weight differs between the original and checkpoint model.\n",
      "Parameter layers.7.mlp.up_proj.weight differs between the original and checkpoint model.\n",
      "Parameter layers.7.mlp.down_proj.weight differs between the original and checkpoint model.\n",
      "Parameter layers.7.input_layernorm.weight differs between the original and checkpoint model.\n",
      "Parameter layers.7.post_attention_layernorm.weight differs between the original and checkpoint model.\n",
      "Parameter layers.8.self_attn.q_proj.weight differs between the original and checkpoint model.\n",
      "Parameter layers.8.self_attn.q_proj.bias differs between the original and checkpoint model.\n",
      "Parameter layers.8.self_attn.k_proj.weight differs between the original and checkpoint model.\n",
      "Parameter layers.8.self_attn.k_proj.bias differs between the original and checkpoint model.\n",
      "Parameter layers.8.self_attn.v_proj.weight differs between the original and checkpoint model.\n",
      "Parameter layers.8.self_attn.v_proj.bias differs between the original and checkpoint model.\n",
      "Parameter layers.8.self_attn.o_proj.weight differs between the original and checkpoint model.\n",
      "Parameter layers.8.mlp.gate_proj.weight differs between the original and checkpoint model.\n",
      "Parameter layers.8.mlp.up_proj.weight differs between the original and checkpoint model.\n",
      "Parameter layers.8.mlp.down_proj.weight differs between the original and checkpoint model.\n",
      "Parameter layers.8.input_layernorm.weight differs between the original and checkpoint model.\n",
      "Parameter layers.8.post_attention_layernorm.weight differs between the original and checkpoint model.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:2 and cuda:1! (when checking argument for argument other in method wrapper_CUDA__equal)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, og_param \u001b[38;5;129;01min\u001b[39;00m og_language_model\u001b[38;5;241m.\u001b[39mnamed_parameters():\n\u001b[1;32m      6\u001b[0m     c_param \u001b[38;5;241m=\u001b[39m c_language_model\u001b[38;5;241m.\u001b[39mget_parameter(name)\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mog_param\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mequal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc_param\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m differs between the original and checkpoint model.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:2 and cuda:1! (when checking argument for argument other in method wrapper_CUDA__equal)"
     ]
    }
   ],
   "source": [
    "og_language_model = og_model.language_model\n",
    "c_language_model = checkpoint_model.language_model\n",
    "\n",
    "# Compare the two models\n",
    "for name, og_param in og_language_model.named_parameters():\n",
    "    c_param = c_language_model.get_parameter(name)\n",
    "    if not og_param.equal(c_param):\n",
    "        print(f\"Parameter {name} differs between the original and checkpoint model.\")\n",
    "    else:\n",
    "        print(f\"Parameter {name} is the same in both models.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed_tokens.weight\n",
      "layers.0.self_attn.q_proj.weight\n",
      "layers.0.self_attn.q_proj.bias\n",
      "layers.0.self_attn.k_proj.weight\n",
      "layers.0.self_attn.k_proj.bias\n",
      "layers.0.self_attn.v_proj.weight\n",
      "layers.0.self_attn.v_proj.bias\n",
      "layers.0.self_attn.o_proj.weight\n",
      "layers.0.mlp.gate_proj.weight\n",
      "layers.0.mlp.up_proj.weight\n",
      "layers.0.mlp.down_proj.weight\n",
      "layers.0.input_layernorm.weight\n",
      "layers.0.post_attention_layernorm.weight\n",
      "layers.1.self_attn.q_proj.weight\n",
      "layers.1.self_attn.q_proj.bias\n",
      "layers.1.self_attn.k_proj.weight\n",
      "layers.1.self_attn.k_proj.bias\n",
      "layers.1.self_attn.v_proj.weight\n",
      "layers.1.self_attn.v_proj.bias\n",
      "layers.1.self_attn.o_proj.weight\n",
      "layers.1.mlp.gate_proj.weight\n",
      "layers.1.mlp.up_proj.weight\n",
      "layers.1.mlp.down_proj.weight\n",
      "layers.1.input_layernorm.weight\n",
      "layers.1.post_attention_layernorm.weight\n",
      "layers.2.self_attn.q_proj.weight\n",
      "layers.2.self_attn.q_proj.bias\n",
      "layers.2.self_attn.k_proj.weight\n",
      "layers.2.self_attn.k_proj.bias\n",
      "layers.2.self_attn.v_proj.weight\n",
      "layers.2.self_attn.v_proj.bias\n",
      "layers.2.self_attn.o_proj.weight\n",
      "layers.2.mlp.gate_proj.weight\n",
      "layers.2.mlp.up_proj.weight\n",
      "layers.2.mlp.down_proj.weight\n",
      "layers.2.input_layernorm.weight\n",
      "layers.2.post_attention_layernorm.weight\n",
      "layers.3.self_attn.q_proj.weight\n",
      "layers.3.self_attn.q_proj.bias\n",
      "layers.3.self_attn.k_proj.weight\n",
      "layers.3.self_attn.k_proj.bias\n",
      "layers.3.self_attn.v_proj.weight\n",
      "layers.3.self_attn.v_proj.bias\n",
      "layers.3.self_attn.o_proj.weight\n",
      "layers.3.mlp.gate_proj.weight\n",
      "layers.3.mlp.up_proj.weight\n",
      "layers.3.mlp.down_proj.weight\n",
      "layers.3.input_layernorm.weight\n",
      "layers.3.post_attention_layernorm.weight\n",
      "layers.4.self_attn.q_proj.weight\n",
      "layers.4.self_attn.q_proj.bias\n",
      "layers.4.self_attn.k_proj.weight\n",
      "layers.4.self_attn.k_proj.bias\n",
      "layers.4.self_attn.v_proj.weight\n",
      "layers.4.self_attn.v_proj.bias\n",
      "layers.4.self_attn.o_proj.weight\n",
      "layers.4.mlp.gate_proj.weight\n",
      "layers.4.mlp.up_proj.weight\n",
      "layers.4.mlp.down_proj.weight\n",
      "layers.4.input_layernorm.weight\n",
      "layers.4.post_attention_layernorm.weight\n",
      "layers.5.self_attn.q_proj.weight\n",
      "layers.5.self_attn.q_proj.bias\n",
      "layers.5.self_attn.k_proj.weight\n",
      "layers.5.self_attn.k_proj.bias\n",
      "layers.5.self_attn.v_proj.weight\n",
      "layers.5.self_attn.v_proj.bias\n",
      "layers.5.self_attn.o_proj.weight\n",
      "layers.5.mlp.gate_proj.weight\n",
      "layers.5.mlp.up_proj.weight\n",
      "layers.5.mlp.down_proj.weight\n",
      "layers.5.input_layernorm.weight\n",
      "layers.5.post_attention_layernorm.weight\n",
      "layers.6.self_attn.q_proj.weight\n",
      "layers.6.self_attn.q_proj.bias\n",
      "layers.6.self_attn.k_proj.weight\n",
      "layers.6.self_attn.k_proj.bias\n",
      "layers.6.self_attn.v_proj.weight\n",
      "layers.6.self_attn.v_proj.bias\n",
      "layers.6.self_attn.o_proj.weight\n",
      "layers.6.mlp.gate_proj.weight\n",
      "layers.6.mlp.up_proj.weight\n",
      "layers.6.mlp.down_proj.weight\n",
      "layers.6.input_layernorm.weight\n",
      "layers.6.post_attention_layernorm.weight\n",
      "layers.7.self_attn.q_proj.weight\n",
      "layers.7.self_attn.q_proj.bias\n",
      "layers.7.self_attn.k_proj.weight\n",
      "layers.7.self_attn.k_proj.bias\n",
      "layers.7.self_attn.v_proj.weight\n",
      "layers.7.self_attn.v_proj.bias\n",
      "layers.7.self_attn.o_proj.weight\n",
      "layers.7.mlp.gate_proj.weight\n",
      "layers.7.mlp.up_proj.weight\n",
      "layers.7.mlp.down_proj.weight\n",
      "layers.7.input_layernorm.weight\n",
      "layers.7.post_attention_layernorm.weight\n",
      "layers.8.self_attn.q_proj.weight\n",
      "layers.8.self_attn.q_proj.bias\n",
      "layers.8.self_attn.k_proj.weight\n",
      "layers.8.self_attn.k_proj.bias\n",
      "layers.8.self_attn.v_proj.weight\n",
      "layers.8.self_attn.v_proj.bias\n",
      "layers.8.self_attn.o_proj.weight\n",
      "layers.8.mlp.gate_proj.weight\n",
      "layers.8.mlp.up_proj.weight\n",
      "layers.8.mlp.down_proj.weight\n",
      "layers.8.input_layernorm.weight\n",
      "layers.8.post_attention_layernorm.weight\n",
      "layers.9.self_attn.q_proj.weight\n",
      "layers.9.self_attn.q_proj.bias\n",
      "layers.9.self_attn.k_proj.weight\n",
      "layers.9.self_attn.k_proj.bias\n",
      "layers.9.self_attn.v_proj.weight\n",
      "layers.9.self_attn.v_proj.bias\n",
      "layers.9.self_attn.o_proj.weight\n",
      "layers.9.mlp.gate_proj.weight\n",
      "layers.9.mlp.up_proj.weight\n",
      "layers.9.mlp.down_proj.weight\n",
      "layers.9.input_layernorm.weight\n",
      "layers.9.post_attention_layernorm.weight\n",
      "layers.10.self_attn.q_proj.weight\n",
      "layers.10.self_attn.q_proj.bias\n",
      "layers.10.self_attn.k_proj.weight\n",
      "layers.10.self_attn.k_proj.bias\n",
      "layers.10.self_attn.v_proj.weight\n",
      "layers.10.self_attn.v_proj.bias\n",
      "layers.10.self_attn.o_proj.weight\n",
      "layers.10.mlp.gate_proj.weight\n",
      "layers.10.mlp.up_proj.weight\n",
      "layers.10.mlp.down_proj.weight\n",
      "layers.10.input_layernorm.weight\n",
      "layers.10.post_attention_layernorm.weight\n",
      "layers.11.self_attn.q_proj.weight\n",
      "layers.11.self_attn.q_proj.bias\n",
      "layers.11.self_attn.k_proj.weight\n",
      "layers.11.self_attn.k_proj.bias\n",
      "layers.11.self_attn.v_proj.weight\n",
      "layers.11.self_attn.v_proj.bias\n",
      "layers.11.self_attn.o_proj.weight\n",
      "layers.11.mlp.gate_proj.weight\n",
      "layers.11.mlp.up_proj.weight\n",
      "layers.11.mlp.down_proj.weight\n",
      "layers.11.input_layernorm.weight\n",
      "layers.11.post_attention_layernorm.weight\n",
      "layers.12.self_attn.q_proj.weight\n",
      "layers.12.self_attn.q_proj.bias\n",
      "layers.12.self_attn.k_proj.weight\n",
      "layers.12.self_attn.k_proj.bias\n",
      "layers.12.self_attn.v_proj.weight\n",
      "layers.12.self_attn.v_proj.bias\n",
      "layers.12.self_attn.o_proj.weight\n",
      "layers.12.mlp.gate_proj.weight\n",
      "layers.12.mlp.up_proj.weight\n",
      "layers.12.mlp.down_proj.weight\n",
      "layers.12.input_layernorm.weight\n",
      "layers.12.post_attention_layernorm.weight\n",
      "layers.13.self_attn.q_proj.weight\n",
      "layers.13.self_attn.q_proj.bias\n",
      "layers.13.self_attn.k_proj.weight\n",
      "layers.13.self_attn.k_proj.bias\n",
      "layers.13.self_attn.v_proj.weight\n",
      "layers.13.self_attn.v_proj.bias\n",
      "layers.13.self_attn.o_proj.weight\n",
      "layers.13.mlp.gate_proj.weight\n",
      "layers.13.mlp.up_proj.weight\n",
      "layers.13.mlp.down_proj.weight\n",
      "layers.13.input_layernorm.weight\n",
      "layers.13.post_attention_layernorm.weight\n",
      "layers.14.self_attn.q_proj.weight\n",
      "layers.14.self_attn.q_proj.bias\n",
      "layers.14.self_attn.k_proj.weight\n",
      "layers.14.self_attn.k_proj.bias\n",
      "layers.14.self_attn.v_proj.weight\n",
      "layers.14.self_attn.v_proj.bias\n",
      "layers.14.self_attn.o_proj.weight\n",
      "layers.14.mlp.gate_proj.weight\n",
      "layers.14.mlp.up_proj.weight\n",
      "layers.14.mlp.down_proj.weight\n",
      "layers.14.input_layernorm.weight\n",
      "layers.14.post_attention_layernorm.weight\n",
      "layers.15.self_attn.q_proj.weight\n",
      "layers.15.self_attn.q_proj.bias\n",
      "layers.15.self_attn.k_proj.weight\n",
      "layers.15.self_attn.k_proj.bias\n",
      "layers.15.self_attn.v_proj.weight\n",
      "layers.15.self_attn.v_proj.bias\n",
      "layers.15.self_attn.o_proj.weight\n",
      "layers.15.mlp.gate_proj.weight\n",
      "layers.15.mlp.up_proj.weight\n",
      "layers.15.mlp.down_proj.weight\n",
      "layers.15.input_layernorm.weight\n",
      "layers.15.post_attention_layernorm.weight\n",
      "layers.16.self_attn.q_proj.weight\n",
      "layers.16.self_attn.q_proj.bias\n",
      "layers.16.self_attn.k_proj.weight\n",
      "layers.16.self_attn.k_proj.bias\n",
      "layers.16.self_attn.v_proj.weight\n",
      "layers.16.self_attn.v_proj.bias\n",
      "layers.16.self_attn.o_proj.weight\n",
      "layers.16.mlp.gate_proj.weight\n",
      "layers.16.mlp.up_proj.weight\n",
      "layers.16.mlp.down_proj.weight\n",
      "layers.16.input_layernorm.weight\n",
      "layers.16.post_attention_layernorm.weight\n",
      "layers.17.self_attn.q_proj.weight\n",
      "layers.17.self_attn.q_proj.bias\n",
      "layers.17.self_attn.k_proj.weight\n",
      "layers.17.self_attn.k_proj.bias\n",
      "layers.17.self_attn.v_proj.weight\n",
      "layers.17.self_attn.v_proj.bias\n",
      "layers.17.self_attn.o_proj.weight\n",
      "layers.17.mlp.gate_proj.weight\n",
      "layers.17.mlp.up_proj.weight\n",
      "layers.17.mlp.down_proj.weight\n",
      "layers.17.input_layernorm.weight\n",
      "layers.17.post_attention_layernorm.weight\n",
      "layers.18.self_attn.q_proj.weight\n",
      "layers.18.self_attn.q_proj.bias\n",
      "layers.18.self_attn.k_proj.weight\n",
      "layers.18.self_attn.k_proj.bias\n",
      "layers.18.self_attn.v_proj.weight\n",
      "layers.18.self_attn.v_proj.bias\n",
      "layers.18.self_attn.o_proj.weight\n",
      "layers.18.mlp.gate_proj.weight\n",
      "layers.18.mlp.up_proj.weight\n",
      "layers.18.mlp.down_proj.weight\n",
      "layers.18.input_layernorm.weight\n",
      "layers.18.post_attention_layernorm.weight\n",
      "layers.19.self_attn.q_proj.weight\n",
      "layers.19.self_attn.q_proj.bias\n",
      "layers.19.self_attn.k_proj.weight\n",
      "layers.19.self_attn.k_proj.bias\n",
      "layers.19.self_attn.v_proj.weight\n",
      "layers.19.self_attn.v_proj.bias\n",
      "layers.19.self_attn.o_proj.weight\n",
      "layers.19.mlp.gate_proj.weight\n",
      "layers.19.mlp.up_proj.weight\n",
      "layers.19.mlp.down_proj.weight\n",
      "layers.19.input_layernorm.weight\n",
      "layers.19.post_attention_layernorm.weight\n",
      "layers.20.self_attn.q_proj.weight\n",
      "layers.20.self_attn.q_proj.bias\n",
      "layers.20.self_attn.k_proj.weight\n",
      "layers.20.self_attn.k_proj.bias\n",
      "layers.20.self_attn.v_proj.weight\n",
      "layers.20.self_attn.v_proj.bias\n",
      "layers.20.self_attn.o_proj.weight\n",
      "layers.20.mlp.gate_proj.weight\n",
      "layers.20.mlp.up_proj.weight\n",
      "layers.20.mlp.down_proj.weight\n",
      "layers.20.input_layernorm.weight\n",
      "layers.20.post_attention_layernorm.weight\n",
      "layers.21.self_attn.q_proj.weight\n",
      "layers.21.self_attn.q_proj.bias\n",
      "layers.21.self_attn.k_proj.weight\n",
      "layers.21.self_attn.k_proj.bias\n",
      "layers.21.self_attn.v_proj.weight\n",
      "layers.21.self_attn.v_proj.bias\n",
      "layers.21.self_attn.o_proj.weight\n",
      "layers.21.mlp.gate_proj.weight\n",
      "layers.21.mlp.up_proj.weight\n",
      "layers.21.mlp.down_proj.weight\n",
      "layers.21.input_layernorm.weight\n",
      "layers.21.post_attention_layernorm.weight\n",
      "layers.22.self_attn.q_proj.weight\n",
      "layers.22.self_attn.q_proj.bias\n",
      "layers.22.self_attn.k_proj.weight\n",
      "layers.22.self_attn.k_proj.bias\n",
      "layers.22.self_attn.v_proj.weight\n",
      "layers.22.self_attn.v_proj.bias\n",
      "layers.22.self_attn.o_proj.weight\n",
      "layers.22.mlp.gate_proj.weight\n",
      "layers.22.mlp.up_proj.weight\n",
      "layers.22.mlp.down_proj.weight\n",
      "layers.22.input_layernorm.weight\n",
      "layers.22.post_attention_layernorm.weight\n",
      "layers.23.self_attn.q_proj.weight\n",
      "layers.23.self_attn.q_proj.bias\n",
      "layers.23.self_attn.k_proj.weight\n",
      "layers.23.self_attn.k_proj.bias\n",
      "layers.23.self_attn.v_proj.weight\n",
      "layers.23.self_attn.v_proj.bias\n",
      "layers.23.self_attn.o_proj.weight\n",
      "layers.23.mlp.gate_proj.weight\n",
      "layers.23.mlp.up_proj.weight\n",
      "layers.23.mlp.down_proj.weight\n",
      "layers.23.input_layernorm.weight\n",
      "layers.23.post_attention_layernorm.weight\n",
      "layers.24.self_attn.q_proj.weight\n",
      "layers.24.self_attn.q_proj.bias\n",
      "layers.24.self_attn.k_proj.weight\n",
      "layers.24.self_attn.k_proj.bias\n",
      "layers.24.self_attn.v_proj.weight\n",
      "layers.24.self_attn.v_proj.bias\n",
      "layers.24.self_attn.o_proj.weight\n",
      "layers.24.mlp.gate_proj.weight\n",
      "layers.24.mlp.up_proj.weight\n",
      "layers.24.mlp.down_proj.weight\n",
      "layers.24.input_layernorm.weight\n",
      "layers.24.post_attention_layernorm.weight\n",
      "layers.25.self_attn.q_proj.weight\n",
      "layers.25.self_attn.q_proj.bias\n",
      "layers.25.self_attn.k_proj.weight\n",
      "layers.25.self_attn.k_proj.bias\n",
      "layers.25.self_attn.v_proj.weight\n",
      "layers.25.self_attn.v_proj.bias\n",
      "layers.25.self_attn.o_proj.weight\n",
      "layers.25.mlp.gate_proj.weight\n",
      "layers.25.mlp.up_proj.weight\n",
      "layers.25.mlp.down_proj.weight\n",
      "layers.25.input_layernorm.weight\n",
      "layers.25.post_attention_layernorm.weight\n",
      "layers.26.self_attn.q_proj.weight\n",
      "layers.26.self_attn.q_proj.bias\n",
      "layers.26.self_attn.k_proj.weight\n",
      "layers.26.self_attn.k_proj.bias\n",
      "layers.26.self_attn.v_proj.weight\n",
      "layers.26.self_attn.v_proj.bias\n",
      "layers.26.self_attn.o_proj.weight\n",
      "layers.26.mlp.gate_proj.weight\n",
      "layers.26.mlp.up_proj.weight\n",
      "layers.26.mlp.down_proj.weight\n",
      "layers.26.input_layernorm.weight\n",
      "layers.26.post_attention_layernorm.weight\n",
      "layers.27.self_attn.q_proj.weight\n",
      "layers.27.self_attn.q_proj.bias\n",
      "layers.27.self_attn.k_proj.weight\n",
      "layers.27.self_attn.k_proj.bias\n",
      "layers.27.self_attn.v_proj.weight\n",
      "layers.27.self_attn.v_proj.bias\n",
      "layers.27.self_attn.o_proj.weight\n",
      "layers.27.mlp.gate_proj.weight\n",
      "layers.27.mlp.up_proj.weight\n",
      "layers.27.mlp.down_proj.weight\n",
      "layers.27.input_layernorm.weight\n",
      "layers.27.post_attention_layernorm.weight\n",
      "layers.28.self_attn.q_proj.weight\n",
      "layers.28.self_attn.q_proj.bias\n",
      "layers.28.self_attn.k_proj.weight\n",
      "layers.28.self_attn.k_proj.bias\n",
      "layers.28.self_attn.v_proj.weight\n",
      "layers.28.self_attn.v_proj.bias\n",
      "layers.28.self_attn.o_proj.weight\n",
      "layers.28.mlp.gate_proj.weight\n",
      "layers.28.mlp.up_proj.weight\n",
      "layers.28.mlp.down_proj.weight\n",
      "layers.28.input_layernorm.weight\n",
      "layers.28.post_attention_layernorm.weight\n",
      "layers.29.self_attn.q_proj.weight\n",
      "layers.29.self_attn.q_proj.bias\n",
      "layers.29.self_attn.k_proj.weight\n",
      "layers.29.self_attn.k_proj.bias\n",
      "layers.29.self_attn.v_proj.weight\n",
      "layers.29.self_attn.v_proj.bias\n",
      "layers.29.self_attn.o_proj.weight\n",
      "layers.29.mlp.gate_proj.weight\n",
      "layers.29.mlp.up_proj.weight\n",
      "layers.29.mlp.down_proj.weight\n",
      "layers.29.input_layernorm.weight\n",
      "layers.29.post_attention_layernorm.weight\n",
      "layers.30.self_attn.q_proj.weight\n",
      "layers.30.self_attn.q_proj.bias\n",
      "layers.30.self_attn.k_proj.weight\n",
      "layers.30.self_attn.k_proj.bias\n",
      "layers.30.self_attn.v_proj.weight\n",
      "layers.30.self_attn.v_proj.bias\n",
      "layers.30.self_attn.o_proj.weight\n",
      "layers.30.mlp.gate_proj.weight\n",
      "layers.30.mlp.up_proj.weight\n",
      "layers.30.mlp.down_proj.weight\n",
      "layers.30.input_layernorm.weight\n",
      "layers.30.post_attention_layernorm.weight\n",
      "layers.31.self_attn.q_proj.weight\n",
      "layers.31.self_attn.q_proj.bias\n",
      "layers.31.self_attn.k_proj.weight\n",
      "layers.31.self_attn.k_proj.bias\n",
      "layers.31.self_attn.v_proj.weight\n",
      "layers.31.self_attn.v_proj.bias\n",
      "layers.31.self_attn.o_proj.weight\n",
      "layers.31.mlp.gate_proj.weight\n",
      "layers.31.mlp.up_proj.weight\n",
      "layers.31.mlp.down_proj.weight\n",
      "layers.31.input_layernorm.weight\n",
      "layers.31.post_attention_layernorm.weight\n",
      "layers.32.self_attn.q_proj.weight\n",
      "layers.32.self_attn.q_proj.bias\n",
      "layers.32.self_attn.k_proj.weight\n",
      "layers.32.self_attn.k_proj.bias\n",
      "layers.32.self_attn.v_proj.weight\n",
      "layers.32.self_attn.v_proj.bias\n",
      "layers.32.self_attn.o_proj.weight\n",
      "layers.32.mlp.gate_proj.weight\n",
      "layers.32.mlp.up_proj.weight\n",
      "layers.32.mlp.down_proj.weight\n",
      "layers.32.input_layernorm.weight\n",
      "layers.32.post_attention_layernorm.weight\n",
      "layers.33.self_attn.q_proj.weight\n",
      "layers.33.self_attn.q_proj.bias\n",
      "layers.33.self_attn.k_proj.weight\n",
      "layers.33.self_attn.k_proj.bias\n",
      "layers.33.self_attn.v_proj.weight\n",
      "layers.33.self_attn.v_proj.bias\n",
      "layers.33.self_attn.o_proj.weight\n",
      "layers.33.mlp.gate_proj.weight\n",
      "layers.33.mlp.up_proj.weight\n",
      "layers.33.mlp.down_proj.weight\n",
      "layers.33.input_layernorm.weight\n",
      "layers.33.post_attention_layernorm.weight\n",
      "layers.34.self_attn.q_proj.weight\n",
      "layers.34.self_attn.q_proj.bias\n",
      "layers.34.self_attn.k_proj.weight\n",
      "layers.34.self_attn.k_proj.bias\n",
      "layers.34.self_attn.v_proj.weight\n",
      "layers.34.self_attn.v_proj.bias\n",
      "layers.34.self_attn.o_proj.weight\n",
      "layers.34.mlp.gate_proj.weight\n",
      "layers.34.mlp.up_proj.weight\n",
      "layers.34.mlp.down_proj.weight\n",
      "layers.34.input_layernorm.weight\n",
      "layers.34.post_attention_layernorm.weight\n",
      "layers.35.self_attn.q_proj.weight\n",
      "layers.35.self_attn.q_proj.bias\n",
      "layers.35.self_attn.k_proj.weight\n",
      "layers.35.self_attn.k_proj.bias\n",
      "layers.35.self_attn.v_proj.weight\n",
      "layers.35.self_attn.v_proj.bias\n",
      "layers.35.self_attn.o_proj.weight\n",
      "layers.35.mlp.gate_proj.weight\n",
      "layers.35.mlp.up_proj.weight\n",
      "layers.35.mlp.down_proj.weight\n",
      "layers.35.input_layernorm.weight\n",
      "layers.35.post_attention_layernorm.weight\n",
      "norm.weight\n"
     ]
    }
   ],
   "source": [
    "for name, mat in og_model.language_model.named_parameters():\n",
    "  print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.enable_input_require_grads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-10 19:56:59,834 - qwenvl.data - INFO - Counting tokens, not training.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from qwenvl.argument import ModelArguments, DataArguments, ProcessingArguments\n",
    "torch.set_num_threads(1)\n",
    "parser = transformers.HfArgumentParser((\n",
    "    ModelArguments,\n",
    "    DataArguments,\n",
    "    ProcessingArguments,\n",
    "))\n",
    "\n",
    "from qwenvl.data import logger\n",
    "from qwenvl.train import set_processor, make_data_module\n",
    "\n",
    "model_args, data_args, proc_args = ModelArguments(), DataArguments(), ProcessingArguments()\n",
    "logger.info(\"Counting tokens, not training.\")\n",
    "processor = transformers.AutoProcessor.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    use_fast=True,\n",
    ")\n",
    "data_args.dataset_use = 'openbiomedvid'\n",
    "processor = set_processor(processor, proc_args, data_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-10 19:57:02,066 - qwenvl.data - INFO - Loading dataset connectthapa84/OpenBiomedVid from /projects/cft_vlm/datasets/openbiomedvid/data/dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-10 19:57:02,087 - qwenvl.data - INFO - bin_pkl_path.exists()=True, self.need_num_content_tokens()=False\n",
      "2025-06-10 19:57:02,089 - qwenvl.data - INFO - Loading bins from pickle file /projects/cft_vlm/datasets/openbiomedvid/data/dataset/bins.pkl.\n"
     ]
    }
   ],
   "source": [
    "data_module = make_data_module(\n",
    "    processor=processor, data_args=data_args, proc_args=proc_args, for_training=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'int' and 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m ds \u001b[38;5;241m=\u001b[39m data_module[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_dataset\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 2\u001b[0m \u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/fs01/projects/cft_vlm/finetune/qwenvl/data/base.py:219\u001b[0m, in \u001b[0;36mBaseDataset.collate_fn\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pack \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[1;32m    218\u001b[0m   batch_convo\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_conversation(pack))\n\u001b[0;32m--> 219\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_model_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_convo\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/fs01/projects/cft_vlm/finetune/qwenvl/data/base.py:201\u001b[0m, in \u001b[0;36mBaseDataset.make_model_input\u001b[0;34m(self, batch_convo)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmake_model_input\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch_convo: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[0;32m--> 201\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmake_model_input\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconversations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_convo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocessor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproc_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfor_training\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfor_training\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/fs01/projects/cft_vlm/finetune/qwenvl/data/utils.py:240\u001b[0m, in \u001b[0;36mmake_model_input\u001b[0;34m(conversations, processor, proc_args, for_training)\u001b[0m\n\u001b[1;32m    234\u001b[0m image_inputs, video_inputs, fpss \u001b[38;5;241m=\u001b[39m get_batch_images_and_videos(\n\u001b[1;32m    235\u001b[0m   conversations, proc_args)\n\u001b[1;32m    237\u001b[0m text \u001b[38;5;241m=\u001b[39m processor\u001b[38;5;241m.\u001b[39mapply_chat_template(\n\u001b[1;32m    238\u001b[0m     conversations, add_generation_prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m for_training, tokenize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    239\u001b[0m )\n\u001b[0;32m--> 240\u001b[0m data_dict \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_inputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimage_inputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvideos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvideo_inputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvideo_inputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfpss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfpss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mright\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Right-pad for flash_attention_2\u001b[39;49;00m\n\u001b[1;32m    248\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m for_training:\n\u001b[1;32m    250\u001b[0m   data_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m make_labels(\n\u001b[1;32m    251\u001b[0m     data_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    252\u001b[0m     processor\u001b[38;5;241m.\u001b[39mtokenizer\n\u001b[1;32m    253\u001b[0m   )\n",
      "File \u001b[0;32m/fs01/projects/cft_vlm/.venv/lib/python3.10/site-packages/transformers/models/qwen2_5_vl/processing_qwen2_5_vl.py:159\u001b[0m, in \u001b[0;36mQwen2_5_VLProcessor.__call__\u001b[0;34m(self, images, text, videos, **kwargs)\u001b[0m\n\u001b[1;32m    157\u001b[0m     second_per_grid_ts \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvideo_processor\u001b[38;5;241m.\u001b[39mtemporal_patch_size \u001b[38;5;241m/\u001b[39m fps] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(video_grid_thw)\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(fps, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__len__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(fps) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(video_grid_thw):\n\u001b[0;32m--> 159\u001b[0m     second_per_grid_ts \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvideo_processor\u001b[38;5;241m.\u001b[39mtemporal_patch_size \u001b[38;5;241m/\u001b[39m tmp \u001b[38;5;28;01mfor\u001b[39;00m tmp \u001b[38;5;129;01min\u001b[39;00m fps]\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    162\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe length of fps (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(fps)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mhasattr\u001b[39m(fps,\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__len__\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39mfps\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) must be equal to the length of video_grid_thw (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(video_grid_thw)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) or fps should be a single number.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    163\u001b[0m     )\n",
      "File \u001b[0;32m/fs01/projects/cft_vlm/.venv/lib/python3.10/site-packages/transformers/models/qwen2_5_vl/processing_qwen2_5_vl.py:159\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    157\u001b[0m     second_per_grid_ts \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvideo_processor\u001b[38;5;241m.\u001b[39mtemporal_patch_size \u001b[38;5;241m/\u001b[39m fps] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(video_grid_thw)\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(fps, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__len__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(fps) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(video_grid_thw):\n\u001b[0;32m--> 159\u001b[0m     second_per_grid_ts \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvideo_processor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtemporal_patch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtmp\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m tmp \u001b[38;5;129;01min\u001b[39;00m fps]\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    162\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe length of fps (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(fps)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mhasattr\u001b[39m(fps,\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__len__\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39mfps\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) must be equal to the length of video_grid_thw (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(video_grid_thw)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) or fps should be a single number.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    163\u001b[0m     )\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'int' and 'list'"
     ]
    }
   ],
   "source": [
    "ds = data_module['train_dataset']\n",
    "ds.collate_fn(ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mpi4py'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 13\u001b[0m\n\u001b[1;32m      9\u001b[0m data_args\u001b[38;5;241m.\u001b[39mdataset_use \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvqa-rad\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     10\u001b[0m data_args\u001b[38;5;241m.\u001b[39msplit \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 13\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mQwen2_5_VLForConditionalGeneration\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_implementation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mflash_attention_2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     17\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m processor \u001b[38;5;241m=\u001b[39m AutoProcessor\u001b[38;5;241m.\u001b[39mfrom_pretrained(eval_args\u001b[38;5;241m.\u001b[39mmodel_name_or_path)\n\u001b[1;32m     19\u001b[0m processor \u001b[38;5;241m=\u001b[39m set_processor(processor, proc_args, data_args)\n",
      "File \u001b[0;32m/fs01/projects/cft_vlm/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py:309\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    307\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    311\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[0;32m/fs01/projects/cft_vlm/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py:4494\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4491\u001b[0m config\u001b[38;5;241m.\u001b[39mname_or_path \u001b[38;5;241m=\u001b[39m pretrained_model_name_or_path\n\u001b[1;32m   4493\u001b[0m \u001b[38;5;66;03m# Instantiate model.\u001b[39;00m\n\u001b[0;32m-> 4494\u001b[0m model_init_context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_init_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43mis_quantized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_is_ds_init_called\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4496\u001b[0m config \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(config)  \u001b[38;5;66;03m# We do not want to modify the config inplace in from_pretrained.\u001b[39;00m\n\u001b[1;32m   4497\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_attn_implementation_autoset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m/fs01/projects/cft_vlm/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py:3881\u001b[0m, in \u001b[0;36mPreTrainedModel.get_init_context\u001b[0;34m(cls, is_quantized, _is_ds_init_called)\u001b[0m\n\u001b[1;32m   3879\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_quantized \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_ds_init_called:\n\u001b[1;32m   3880\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDetected DeepSpeed ZeRO-3: activating zero.init() for this model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 3881\u001b[0m     init_contexts\u001b[38;5;241m.\u001b[39mextend([\u001b[43mdeepspeed\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_dict_or_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeepspeed_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m, set_zero3_state()])\n\u001b[1;32m   3882\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_quantized:\n\u001b[1;32m   3883\u001b[0m     init_contexts\u001b[38;5;241m.\u001b[39mextend([init_empty_weights(), set_quantized_state()])\n",
      "File \u001b[0;32m/fs01/projects/cft_vlm/.venv/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py:956\u001b[0m, in \u001b[0;36mInit.__init__\u001b[0;34m(self, module, data_parallel_group, mem_efficient_linear, remote_device, pin_memory, config_dict_or_path, config, enabled, dtype, mpu, zero_param_parallel_group, zero_quantized_weights, zero_quantized_nontrainable_weights, sequence_data_parallel_group, param_swapper)\u001b[0m\n\u001b[1;32m    954\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(enabled\u001b[38;5;241m=\u001b[39menabled, mem_efficient_linear\u001b[38;5;241m=\u001b[39mmem_efficient_linear, ds_config\u001b[38;5;241m=\u001b[39m_ds_config, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    955\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dist\u001b[38;5;241m.\u001b[39mis_initialized():\n\u001b[0;32m--> 956\u001b[0m     \u001b[43minit_distributed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    957\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m dist\u001b[38;5;241m.\u001b[39mis_initialized(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameters cannot be scattered without initializing deepspeed.comm\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    959\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data_parallel_group \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/fs01/projects/cft_vlm/.venv/lib/python3.10/site-packages/deepspeed/comm/comm.py:690\u001b[0m, in \u001b[0;36minit_distributed\u001b[0;34m(dist_backend, auto_mpi_discovery, distributed_port, verbose, timeout, init_method, dist_init_required, config, rank, world_size)\u001b[0m\n\u001b[1;32m    688\u001b[0m         patch_aws_sm_env_for_torch_nccl_backend(verbose\u001b[38;5;241m=\u001b[39mverbose)\n\u001b[1;32m    689\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 690\u001b[0m         \u001b[43mmpi_discovery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdistributed_port\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdistributed_port\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cdb \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m cdb\u001b[38;5;241m.\u001b[39mis_initialized():\n\u001b[1;32m    693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mint\u001b[39m(os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRANK\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/fs01/projects/cft_vlm/.venv/lib/python3.10/site-packages/deepspeed/comm/comm.py:709\u001b[0m, in \u001b[0;36mmpi_discovery\u001b[0;34m(distributed_port, verbose)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmpi_discovery\u001b[39m(distributed_port\u001b[38;5;241m=\u001b[39mTORCH_DISTRIBUTED_DEFAULT_PORT, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    706\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    707\u001b[0m \u001b[38;5;124;03m    Discovery MPI environment via mpi4py and map to relevant dist state\u001b[39;00m\n\u001b[1;32m    708\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 709\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmpi4py\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MPI\n\u001b[1;32m    710\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msubprocess\u001b[39;00m\n\u001b[1;32m    711\u001b[0m     comm \u001b[38;5;241m=\u001b[39m MPI\u001b[38;5;241m.\u001b[39mCOMM_WORLD\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mpi4py'"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "from qwenvl.new.predict import get_trainer_args, get_generated_text, save_result\n",
    "from qwenvl.new.train import rank0_make_data_module\n",
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "data_args, proc_args, eval_args = DataArguments(), ProcessingArguments(), EvalArguments()\n",
    "\n",
    "data_args.dataset_use = 'vqa-rad'\n",
    "data_args.split = 'test'\n",
    "\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    eval_args.model_name_or_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    attn_implementation=\"flash_attention_2\"\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(eval_args.model_name_or_path)\n",
    "processor = set_processor(processor, proc_args, data_args)\n",
    "data_module = rank0_make_data_module(\n",
    "    processor=processor,\n",
    "    data_args=data_args,\n",
    "    proc_args=proc_args,\n",
    "    for_training=False\n",
    ")\n",
    "trainer_args = get_trainer_args(eval_args)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    processing_class=processor,\n",
    "    args=trainer_args,\n",
    "    **data_module\n",
    ")\n",
    "output = trainer.predict(trainer.eval_dataset)\n",
    "generated_text = get_generated_text(output, processor.tokenizer)\n",
    "if eval_args.output_dir is not None:\n",
    "  output_dir = Path(eval_args.output_dir)\n",
    "else:\n",
    "  output_dir = Path(eval_args.model_name_or_path)\n",
    "save_result(data_module['eval_dataset'], generated_text, output_dir)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qwenvl.data import OpenpmcDataset\n",
    "\n",
    "\n",
    "ds = OpenpmcDataset(\n",
    "    processor=processor,\n",
    "    data_args=data_args,\n",
    "    proc_args=proc_args,\n",
    "    split='train',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3126e62bd0b8403ba996ffbda26fcfbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs01/projects/cft_vlm/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:2479: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration\n",
    "\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-VL-3B-Instruct\",\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\", \n",
    "    attn_implementation=\"flash_attention_2\"\n",
    ")\n",
    "from PIL import Image as PILImage\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-3B-Instruct\", use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = PILImage.fromarray((np.ones((224, 224, 3)) * np.array([0, 0, 255])).astype(np.uint8))\n",
    "img2 = PILImage.fromarray((np.ones((224, 224, 3)) * np.array([0, 255, 0])).astype(np.uint8))\n",
    "conversations = [\n",
    "  [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": 'image',\n",
    "          \"image\": img1,\n",
    "        },\n",
    "        {\n",
    "          \"type\": 'text',\n",
    "          \"text\": \"What is the color of this image?\"\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "  ],\n",
    "  [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": 'image',\n",
    "          \"image\": img2,\n",
    "        },\n",
    "        {\n",
    "          \"type\": 'text',\n",
    "          \"text\": \"What is the color of this image?\"\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "]\n",
    "texts = processor.tokenizer.apply_chat_template(\n",
    "    conversations[0],\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "inputs = processor(\n",
    "    images=[[img1], [img2]][0],\n",
    "    text=texts,\n",
    "    return_tensors=\"pt\",\n",
    "    padding_side='right',\n",
    ")\n",
    "output = model.generate(**inputs, max_new_tokens=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['system\\nYou are a helpful assistant.\\nuser\\nWhat is the color of this image?\\nassistant\\nThe image you provided is a solid blue color.']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.tokenizer.batch_decode(output, skip_special_tokens=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cft_vlm",
   "language": "python",
   "name": "cft_vlm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
