{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc9bab383baa46c8b6724ab0bb1ca6ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1191 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c11e9173bcac419ca50f3fa889a28b49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1053 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import datasets\n",
    "from pathlib import Path\n",
    "\n",
    "benchmark_dir = Path(os.environ['BENCHMARK_DIR'])\n",
    "\n",
    "ds_key = 'flaviagiammarino/vqa-rad'\n",
    "ds_name = ds_key.split('/')[-1]\n",
    "ds = datasets.load_dataset(ds_key)\n",
    "\n",
    "open_ended = list()\n",
    "yes_no = list()\n",
    "\n",
    "for split_name, split in ds.items():\n",
    "  for item in split:\n",
    "    item['split'] = split_name\n",
    "    item['media'] = [item.pop('image')]\n",
    "    \n",
    "    if item['answer'].strip() in set(['yes', 'no']):\n",
    "      item['answer'] = item['answer'].strip()[0].upper()\n",
    "      yes_no.append(item)\n",
    "    else:\n",
    "      open_ended.append(item)\n",
    "\n",
    "for idx, item in enumerate(yes_no):\n",
    "    item['id'] = idx\n",
    "for idx, item in enumerate(open_ended):\n",
    "    item['id'] = idx\n",
    "\n",
    "ds_yes_no = datasets.Dataset.from_list(yes_no)\n",
    "ds_open_ended = datasets.Dataset.from_list(open_ended)\n",
    "\n",
    "ds_yes_no.save_to_disk(benchmark_dir  / ds_name / 'yes-no' /'dataset')\n",
    "ds_open_ended.save_to_disk(benchmark_dir / ds_name / 'open-ended'/'dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84a5eb4bd5d64b5c84043adc39e6fdde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    \"/model-weights/Qwen2.5-VL-7B-Instruct\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\", use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: [{'role': 'user', 'content': [{'type': 'image', 'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=480x503 at 0x7FB0D41509D0>}, {'type': 'text', 'text': 'are the lungs normal appearing?\\nYou only options are:\\nY\\nN\\nPlease answer with exactly one letter chosen from the options above.'}]}]\n",
      "Input text: <|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "<|vision_start|><|image_pad|><|vision_end|>are the lungs normal appearing?\n",
      "You only options are:\n",
      "Y\n",
      "N\n",
      "Please answer with exactly one letter chosen from the options above.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "['N<|im_end|>']\n"
     ]
    }
   ],
   "source": [
    "del make_question\n",
    "\n",
    "from infer import make_question\n",
    "\n",
    "question, text = make_question(ds_yes_no[1], processor)\n",
    "image_inputs, video_inputs = process_vision_info(question)\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=32)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=False, clean_up_tokenization_spaces=False\n",
    ")\n",
    "print(\"Question:\", question)\n",
    "print(\"Input text:\", text)\n",
    "print(output_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'are the lungs normal appearing?',\n",
       " 'answer': 'N',\n",
       " 'split': 'train',\n",
       " 'media': [<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=480x503>],\n",
       " 'id': 1}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_yes_no[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|>Describe this image.<|im_end|>\\n<|im_start|>assistant\\n']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.batch_decode(inputs['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cft_vlm",
   "language": "python",
   "name": "cft_vlm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
